# GPULlama3 Configuration
quarkus.langchain4j.gpu-llama3.enable-integration=true
quarkus.langchain4j.gpu-llama3.chat-model.model-name=unsloth/Llama-3.2-1B-Instruct-GGUF
quarkus.langchain4j.gpu-llama3.chat-model.quantization=F16
quarkus.langchain4j.gpu-llama3.chat-model.temperature=0.7
quarkus.langchain4j.gpu-llama3.chat-model.max-tokens=1024

# Selective model creation
quarkus.langchain4j.gpu-llama3.model-creation.create-chat-model=true
quarkus.langchain4j.gpu-llama3.model-creation.create-streaming-chat-model=false