# streaming-demo/src/main/resources/application.properties
quarkus.package.main-class=org.example._1_summarization_service.MainStreamingChatModel

# GPULlama3 Configuration - STREAMING ONLY
quarkus.langchain4j.gpu-llama3.enable-integration=true

# GPULlama3 Configuration
quarkus.langchain4j.gpu-llama3.chat-model.model-name=unsloth/Llama-3.2-1B-Instruct-GGUF
quarkus.langchain4j.gpu-llama3.chat-model.quantization=F16
quarkus.langchain4j.gpu-llama3.chat-model.temperature=0.7
quarkus.langchain4j.gpu-llama3.chat-model.max-tokens=1024